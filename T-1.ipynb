{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b025dcb-8e9c-4bc2-a16b-50131aeab08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd, matplotlib.pyplot as plt, openai\n",
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cd1d778-8049-4a5f-af04-8c993c80b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade0737d-9225-4ad4-8f59-9d7c0cd79b2c",
   "metadata": {},
   "source": [
    "## Loading the PDF in chunks with Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddc4a14-75ca-458a-b77f-880bcb32f115",
   "metadata": {},
   "source": [
    "### Split by pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93f3814a-702b-4271-9942-6403690e604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('Attention_is_all_you_need.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b856876-00b4-4a6d-a5cd-dc7b40c6950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca58f3b4-b591-4d48-ba05-c2d5da2ab2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Attention Is All You Need\\nAshish Vaswani\\x03\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\x03\\nGoogle Brain\\nnoam@google.comNiki Parmar\\x03\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\x03\\nGoogle Research\\nusz@google.com\\nLlion Jones\\x03\\nGoogle Research\\nllion@google.comAidan N. Gomez\\x03y\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser\\x03\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\x03z\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\nyWork performed while at Google Brain.\\nzWork performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v5  [cs.CL]  6 Dec 2017' metadata={'source': 'Attention_is_all_you_need.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61148b8c-fe47-4449-87e4-a048949500a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d07222c0-a4b8-4ce6-baf9-3bafe4f1ac4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7be91b3-4829-452a-aee5-ffc42eb5f87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.schema.Document"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97932052-c10f-4a1e-a728-1fb71a9675d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc545072-317e-49c9-938d-866d52ec5bc7",
   "metadata": {},
   "source": [
    "## 2) Embedding Text and Store embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbfbfac7-37c8-4c30-8342-0ac475dbae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings() # get embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d7e99ac-c49a-4051-ba69-088abebab6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(chunks, embeddings) # Create vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053a95d5-b81d-4798-b3e6-99d90f764258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa7780d3-8d48-4715-922f-554caa11caa3",
   "metadata": {},
   "source": [
    "## 3) Setup retrieval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2744bab8-efec-43c6-9bcf-d4f2794feba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who created transformers?\"\n",
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06dfd2ca-6d1b-4f7f-81c0-496df8530467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d679160-d087-4df4-a2b1-ab5a05e42ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Attention Is All You Need\\nAshish Vaswani\\x03\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\x03\\nGoogle Brain\\nnoam@google.comNiki Parmar\\x03\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\x03\\nGoogle Research\\nusz@google.com\\nLlion Jones\\x03\\nGoogle Research\\nllion@google.comAidan N. Gomez\\x03y\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser\\x03\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\x03z\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\nyWork performed while at Google Brain.\\nzWork performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v5  [cs.CL]  6 Dec 2017' metadata={'source': 'Attention_is_all_you_need.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "555bfb8a-8308-4f23-87cf-aee2915578a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Attention Is All You Need\\nAshish Vaswani\\x03\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\x03\\nGoogle Brain\\nnoam@google.comNiki Parmar\\x03\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\x03\\nGoogle Research\\nusz@google.com\\nLlion Jones\\x03\\nGoogle Research\\nllion@google.comAidan N. Gomez\\x03y\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser\\x03\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\x03z\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\nyWork performed while at Google Brain.\\nzWork performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v5  [cs.CL]  6 Dec 2017', metadata={'source': 'Attention_is_all_you_need.pdf', 'page': 0}),\n",
       " Document(page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdrop\\x0flstrain PPL BLEU params\\nsteps (dev) (dev)\\x02106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents speciﬁc challenges: the output is subject to strong structural\\n9', metadata={'source': 'Attention_is_all_you_need.pdf', 'page': 8}),\n",
       " Document(page_content='transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstatesht, as a function of the previous hidden state ht\\x001and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1;:::;x n)to a sequence\\nof continuous representations z= (z1;:::;z n). Given z, the decoder then generates an output\\nsequence (y1;:::;y m)of symbols one element at a time. At each step the model is auto-regressive', metadata={'source': 'Attention_is_all_you_need.pdf', 'page': 1}),\n",
       " Document(page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1:0\\x011020\\nGNMT + RL [38] 24.6 39.92 2:3\\x0110191:4\\x011020\\nConvS2S [9] 25.16 40.46 9:6\\x0110181:5\\x011020\\nMoE [32] 26.03 40.56 2:0\\x0110191:2\\x011020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8:0\\x011020\\nGNMT + RL Ensemble [38] 26.30 41.16 1:8\\x0110201:1\\x011021\\nConvS2S Ensemble [9] 26.36 41.29 7:7\\x0110191:2\\x011021\\nTransformer (base model) 27.3 38.1 3:3\\x011018\\nTransformer (big) 28.4 41.8 2:3\\x011019\\nLabel Smoothing During training, we employed label smoothing of value \\x0fls= 0:1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0\\nBLEU, establishing a new state-of-the-art BLEU score of 28:4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3:5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0,\\noutperforming all of the previously published single models, at less than 1=4the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0:1, instead of 0:3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty \\x0b= 0:6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', metadata={'source': 'Attention_is_all_you_need.pdf', 'page': 7})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e8ec2-40f0-4c00-9772-134890baca9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cfe74ed-0d56-466f-ac6c-d9576f4e19e1",
   "metadata": {},
   "source": [
    "### Create Q&A chain to integrate similarity search with user queries (answer queries from Knowledge Base Articles (KBAs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a67811a-3654-475d-b262-fc691eaa7152",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(OpenAI(temperature=0), chain_type='stuff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05e86735-fcb1-4e79-8e61-4ea148f25baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, and Łukasz Kaiser created transformers.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who created transformers?\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "chain.run(input_documents=docs, question = query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97bed4e6-9037-42ac-bcb7-e033c3964cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, and Łukasz Kaiser created transformers.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = str(chain.run(input_documents=docs, question = query)).strip()\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28c6b527-5afb-46a8-8316-c17134231f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter question:  who built transformers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin built the Transformer.\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter question: \")\n",
    "loader = PyPDFLoader('Attention_is_all_you_need.pdf')\n",
    "pages = loader.load_and_split()\n",
    "chunks = pages\n",
    "embeddings = OpenAIEmbeddings() # get embedding model\n",
    "db = FAISS.from_documents(chunks, embeddings) # Create vector database\n",
    "docs = db.similarity_search(query)\n",
    "chain = load_qa_chain(OpenAI(temperature=0), chain_type='stuff') # setting up the chain instance\n",
    "chain.run(input_documents=docs, question = query) # executing the chain\n",
    "answer = str(chain.run(input_documents=docs, question = query)).strip()\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec829a32-fa4a-4b90-adfc-dad5721d77f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36ef38-2a4e-48e3-9c84-58a51f0961f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68342383-193a-4c80-a883-aea6f8cbc52f",
   "metadata": {},
   "source": [
    "## Create chatbot with chat memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1913476-3899-42ed-868b-c6dc81de3a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display \n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b80dda6-cdb0-488d-94b7-f57c62b64359",
   "metadata": {},
   "source": [
    "### Create conversation chain that uses our vectordb as retriever, this also allows for chat history management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ce14601-f91f-4d6b-8438-62d989fa0c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), db.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "912ba997-fa93-4ca1-9de3-a2f6cc4dec0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_23524\\4189155171.py:20: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  input_box.on_submit(on_submit)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58124c4c9f7b4e36aacf7f19f5ba8fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Please enter your question: ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "def on_submit(_):\n",
    "    query = input_box.value\n",
    "    input_box.value = \"\"\n",
    "    \n",
    "    if query.lower() == 'exit':\n",
    "        print('Thanks!!')\n",
    "        return \n",
    "    \n",
    "    result = qa({\"question\":query, \"chat_history\":chat_history})\n",
    "    chat_history.append((query, result['answer']))\n",
    "    \n",
    "    display(widgets.HTML(f'<b>User: <b> {query}'))\n",
    "    display(widgets.HTML(f\"<b><font color='blue'> Chatbot: </font></b> {result['answer']}\"))\n",
    "    \n",
    "print(\"Welcome!!\")\n",
    "\n",
    "input_box = widgets.Text(placeholder = 'Please enter your question: ')\n",
    "input_box.on_submit(on_submit)\n",
    "\n",
    "display(input_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc97f38-d54f-4382-89b7-1cde84ddf0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "cc469579-3c44-49b8-a888-900277e7cfe5",
   "metadata": {
    "id": "iADY2CXNlNq9"
   },
   "source": [
    "# Advanced method - Split by chunk\n",
    "\n",
    "# Step 1: Convert PDF to text\n",
    "import textract\n",
    "doc = textract.process(\"./attention_is_all_you_need.pdf\")\n",
    "\n",
    "# Step 2: Save to .txt and reopen (helps prevent issues)\n",
    "with open('attention_is_all_you_need.txt', 'w') as f:\n",
    "    f.write(doc.decode('utf-8'))\n",
    "\n",
    "with open('attention_is_all_you_need.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Step 3: Create function to count tokens\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Step 4: Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap  = 24,\n",
    "    length_function = count_tokens,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "388d6b11-3579-43e1-92a0-a46e9b86c152",
   "metadata": {
    "id": "KQ_gDkwep4q7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.schema.Document"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result is many LangChain 'Documents' around 500 tokens or less (Recursive splitter sometimes allows more tokens to retain context)\n",
    "type(chunks[0]) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "95268665-af73-466b-a5b3-8d2c494a7667",
   "metadata": {
    "id": "fK31bxDOpz1l"
   },
   "source": [
    "# Quick data visualization to ensure chunking was successful\n",
    "\n",
    "# Create a list of token counts\n",
    "token_counts = [count_tokens(chunk.page_content) for chunk in chunks]\n",
    "\n",
    "# Create a DataFrame from the token counts\n",
    "df = pd.DataFrame({'Token Count': token_counts})\n",
    "\n",
    "# Create a histogram of the token count distribution\n",
    "df.hist(bins=40, )\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
